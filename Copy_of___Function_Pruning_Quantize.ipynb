{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIqCI4Ib5LbGQ9p8NRmLaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierredantas/LLMCompress/blob/main/Copy_of___Function_Pruning_Quantize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5mEp2etDyYX_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "from transformers import BertModel, BertConfig\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that loads a neural network model and creates a copy for processing.\n",
        "- Inputs:\n",
        "    - model_name (str): Path or name of model to load. Default is 'bert-base-uncased'\n",
        "    - model_type (str): Type of model to load ('bert' or 'pytorch'). Default is 'bert'\n",
        "- Outputs:\n",
        "    - original_model: The loaded model instance\n",
        "    - model_copy: Deep copy of the loaded model\n",
        "\n",
        "The function:\n",
        "1. Loads model based on specified type:\n",
        "    - BERT models using BertConfig and BertModel\n",
        "    - PyTorch models using torch.load\n",
        "2. Creates deep copy of loaded model\n",
        "3. Returns both original and copied models\n",
        "\n",
        "Model loading adapts to different model architectures while preserving the original model through copying."
      ],
      "metadata": {
        "id": "0ETd1BNk0HSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name='bert-base-uncased', model_type='bert'):\n",
        "\n",
        "    # Load model based on type\n",
        "    if model_type.lower() == 'bert':\n",
        "        config = BertConfig.from_pretrained(model_name)\n",
        "        base_model = BertModel(config)\n",
        "    elif model_type.lower() == 'pytorch':\n",
        "        base_model = torch.load(model_name)\n",
        "    else:\n",
        "        raise ValueError(f\"Model type {model_type} not supported\")\n",
        "\n",
        "    # Create a deep copy of the model\n",
        "    base_model_copy = copy.deepcopy(base_model)\n",
        "\n",
        "    return base_model, base_model_copy"
      ],
      "metadata": {
        "id": "eIZNCCsTRzJz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that calculates the percentage of zero-valued parameters in a neural network model.\n",
        "- Inputs:\n",
        "    - model: Any neural network model to analyze\n",
        "- Outputs:\n",
        "    - global_sparsity (float): Percentage of zero-valued weights in the model\n",
        "\n",
        "The function:\n",
        "1. Initializes counters for zeros and total elements\n",
        "2. Iterates through all named weight parameters in the model\n",
        "3. For each weight parameter:\n",
        "    - Counts number of zero values\n",
        "    - Counts total number of elements\n",
        "    - Accumulates these counts\n",
        "4. Calculates and returns global sparsity percentage (zeros/total * 100)\n",
        "\n",
        "Model sparsity represents the proportion of weight parameters that are exactly zero, indicating how much the model has been pruned or sparsified."
      ],
      "metadata": {
        "id": "LhdF3A5V4llT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sparsity(model):\n",
        "    # Initialize counters\n",
        "    total_zeros = 0\n",
        "    total_elements = 0\n",
        "\n",
        "    # Calculate for all model parameters\n",
        "    print(\"\\nLayer-wise sparsity:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:  # Only count weight parameters\n",
        "            # Convert boolean comparison directly to float tensor\n",
        "            zeros = (param == 0).float().sum().item()\n",
        "            elements = param.nelement()\n",
        "            layer_sparsity = 100. * zeros / elements\n",
        "            print(f\"Sparsity in {name}: {layer_sparsity:.2f}%\")\n",
        "\n",
        "            # Accumulate for global sparsity\n",
        "            total_zeros += zeros\n",
        "            total_elements += elements\n",
        "\n",
        "    # Calculate and return global sparsity\n",
        "    global_sparsity = 100. * total_zeros / total_elements\n",
        "\n",
        "    return global_sparsity"
      ],
      "metadata": {
        "id": "WVIig_r1FJwx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that loads a saved neural network model and generates comprehensive statistics about its parameters and structure.\n",
        "- Inputs:\n",
        "    - model_path (str): Path to the saved model file\n",
        "    - model_type (str): Type of model to load ('bert' or 'pytorch')\n",
        "    - model_name (str, optional): Name identifier for the model. Default is \"Model\"\n",
        "    - save_stats_path (str, optional): Path to save statistics to file. Default is None\n",
        "- Outputs:\n",
        "    - None (prints statistics and optionally saves to file)\n",
        "\n",
        "The function:\n",
        "1. Loads the model:\n",
        "    - Handles different model types (BERT or PyTorch)\n",
        "    - Validates model type support\n",
        "2. Calculates model metrics:\n",
        "    - Counts total and trainable parameters\n",
        "    - Computes model size in MB\n",
        "    - Determines model sparsity\n",
        "3. Generates statistics report:\n",
        "    - Model identification information\n",
        "    - Parameter counts and size\n",
        "    - Sparsity measurements\n",
        "4. Outputs results:\n",
        "    - Prints statistics to console\n",
        "    - Optionally saves to specified file path\n",
        "\n",
        "Model statistics provide insights into model complexity, memory usage, and compression through sparsity."
      ],
      "metadata": {
        "id": "b0ufOO9b7AZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_stats(model, model_name=\"Model\"):\n",
        "\n",
        "    # Calculate number of parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Calculate non-zero parameters\n",
        "    nonzero_params = sum(torch.count_nonzero(p) for name, p in model.named_parameters() if 'weight' in name)\n",
        "\n",
        "    # Calculate model size considering element size of parameters\n",
        "    model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "\n",
        "    # Calculate sparsity\n",
        "    sparsity = calculate_sparsity(model)\n",
        "\n",
        "    # Create statistics string\n",
        "    stats = f\"=== {model_name} Statistics ===\\n\"\n",
        "    stats += f\"Total parameters: {total_params:,}\\n\"\n",
        "    stats += f\"Trainable parameters: {trainable_params:,}\\n\"\n",
        "    stats += f\"Non-zero parameters: {nonzero_params:,}\\n\"\n",
        "    stats += f\"Model size: {model_size:.2f} MB\\n\"\n",
        "    stats += f\"Model sparsity: {sparsity:.2f}%\\n\"\n",
        "    stats += \"=\"*25\n",
        "\n",
        "    # Print statistics\n",
        "    print(stats)"
      ],
      "metadata": {
        "id": "KUY85PHk3aeH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that applies global unstructured pruning to a PyTorch model using L1 norm and saves the pruned model.\n",
        "- Inputs:\n",
        "    - model: PyTorch model to be pruned\n",
        "    - amount (float, optional): Fraction of parameters to prune, range 0 to 1.\n",
        "    - save_path (str, optional): Directory path where pruned model will be saved. Default is 'pruned_model'\n",
        "- Outputs:\n",
        "    - model: The pruned PyTorch model\n",
        "    - save_path: Path where the pruned model was saved\n",
        "\n",
        "The function:\n",
        "1. Gets prunable parameters from model using get_prunable_parameters()\n",
        "2. Applies global unstructured L1 pruning with specified amount\n",
        "3. Creates directory and saves pruned model to disk\n",
        "4. Returns pruned model and save location\n",
        "\n",
        "Global unstructured pruning removes weights based on their L1 norm magnitude across the entire network."
      ],
      "metadata": {
        "id": "Et3aGShk29AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_model(model, amount):\n",
        "\n",
        "    # Get parameters that can be pruned\n",
        "    parameters_to_prune = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            parameters_to_prune.append((module, 'weight'))\n",
        "\n",
        "    # Apply global unstructured pruning using the simpler version\n",
        "    prune.global_unstructured(\n",
        "        parameters_to_prune,\n",
        "        pruning_method=prune.L1Unstructured,  # Use the class directly\n",
        "        amount=amount\n",
        "    )\n",
        "\n",
        "    # Make the pruning permanent\n",
        "    for module, name in parameters_to_prune:\n",
        "        prune.remove(module, 'weight')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TetLJzI-8zMG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to float16 (quantization)"
      ],
      "metadata": {
        "id": "MtqaVDqdkpD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_float16(model):\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Create a copy of the model first\n",
        "        model_fp16 = copy.deepcopy(model)\n",
        "\n",
        "        # Convert model to Float16\n",
        "        model_fp16 = model_fp16.half()\n",
        "\n",
        "        return model_fp16\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Float16 conversion: {str(e)}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "IqBArutUkjrd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load BERT model and show the statistics"
      ],
      "metadata": {
        "id": "HaUEIjj2VKv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT model\n",
        "print(\"Loading BERT model...\")\n",
        "original_model, original_model_copy = load_model(model_name='bert-base-uncased', model_type='bert')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Print model statistics\n",
        "print_model_stats(\n",
        "    model=original_model,\n",
        "    model_name=\"BERT Base Uncased\",\n",
        ")"
      ],
      "metadata": {
        "id": "pmYQOT4wSbqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8591d8dc-5ff6-4187-b952-c241f0a3860c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT model...\n",
            "Model loaded successfully!\n",
            "\n",
            "Layer-wise sparsity:\n",
            "--------------------------------------------------\n",
            "Sparsity in embeddings.word_embeddings.weight: 0.00%\n",
            "Sparsity in embeddings.position_embeddings.weight: 0.00%\n",
            "Sparsity in embeddings.token_type_embeddings.weight: 0.00%\n",
            "Sparsity in embeddings.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.attention.self.query.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.attention.self.key.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.attention.self.value.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.attention.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.intermediate.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.output.dense.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in pooler.dense.weight: 0.00%\n",
            "=== BERT Base Uncased Statistics ===\n",
            "Total parameters: 109,482,240\n",
            "Trainable parameters: 109,482,240\n",
            "Non-zero parameters: 109,378,545\n",
            "Model size: 417.64 MB\n",
            "Model sparsity: 0.00%\n",
            "=========================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me explain the implications of pruning different types of layers in BERT:\n",
        "\n",
        "- Embedding Layers:\n",
        "  - Risky to prune extensively as they map vocabulary tokens to dense representations\n",
        "  - Heavy pruning could severely impact the model's ability to understand word meanings\n",
        "  - Each zero in embedding means a word loses some of its semantic features\n",
        "\n",
        "- LayerNorm Parameters:\n",
        "  - These are crucial for stabilizing network training\n",
        "  - Very few parameters compared to other layers\n",
        "  - Pruning these could destabilize the entire network\n",
        "  - Generally not recommended to prune normalization layers\n",
        "\n",
        "- Bias Terms:\n",
        "  - Biases add important offsets to each neuron's activation\n",
        "  - Relatively few parameters compared to weights\n",
        "  - Pruning biases can significantly impact model performance\n",
        "  - Usually kept intact as their memory footprint is small\n",
        "\n",
        "\n",
        "- Other Layer Types:\n",
        "  - Attention layers: Pruning could damage the model's ability to focus on relevant parts of input\n",
        "  - Position embeddings: Pruning would hurt the model's understanding of word order\n",
        "  - Intermediate layers: Could be pruned but might affect complex feature representations\n",
        "\n",
        "- That's why the common practice is to:\n",
        "  - Focus pruning on Linear/Conv2d layers as they have the most redundant parameters\n",
        "  - Leave embedding, normalization, and bias terms intact\n",
        "  - Preserve the model's fundamental abilities while reducing size"
      ],
      "metadata": {
        "id": "32VmuECWLXd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prune the model\n",
        "print(\"\\nPruning model...\")\n",
        "pruning_amount = 0.2\n",
        "pruned_model = prune_model(model=original_model, amount=pruning_amount)\n",
        "print(\"Model pruned successfully!\")\n",
        "\n",
        "# Print model statistics\n",
        "print(\"\\nGenerating pruned model statistics...\")\n",
        "print_model_stats(\n",
        "    model=pruned_model,\n",
        "    model_name=\"Pruned BERT\",\n",
        ")"
      ],
      "metadata": {
        "id": "xXniUMB75DBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c1d898-c5e0-458d-b9a5-3b3e0a7149c8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pruning model...\n",
            "Model pruned successfully!\n",
            "\n",
            "Generating pruned model statistics...\n",
            "\n",
            "Layer-wise sparsity:\n",
            "--------------------------------------------------\n",
            "Sparsity in embeddings.word_embeddings.weight: 0.00%\n",
            "Sparsity in embeddings.position_embeddings.weight: 0.00%\n",
            "Sparsity in embeddings.token_type_embeddings.weight: 0.00%\n",
            "Sparsity in embeddings.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.attention.self.query.weight: 19.98%\n",
            "Sparsity in encoder.layer.0.attention.self.key.weight: 19.99%\n",
            "Sparsity in encoder.layer.0.attention.self.value.weight: 19.94%\n",
            "Sparsity in encoder.layer.0.attention.output.dense.weight: 20.05%\n",
            "Sparsity in encoder.layer.0.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.0.output.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.0.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.attention.self.query.weight: 19.95%\n",
            "Sparsity in encoder.layer.1.attention.self.key.weight: 20.06%\n",
            "Sparsity in encoder.layer.1.attention.self.value.weight: 20.01%\n",
            "Sparsity in encoder.layer.1.attention.output.dense.weight: 19.93%\n",
            "Sparsity in encoder.layer.1.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.intermediate.dense.weight: 20.02%\n",
            "Sparsity in encoder.layer.1.output.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.1.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.attention.self.query.weight: 20.02%\n",
            "Sparsity in encoder.layer.2.attention.self.key.weight: 20.04%\n",
            "Sparsity in encoder.layer.2.attention.self.value.weight: 20.00%\n",
            "Sparsity in encoder.layer.2.attention.output.dense.weight: 19.92%\n",
            "Sparsity in encoder.layer.2.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.intermediate.dense.weight: 19.98%\n",
            "Sparsity in encoder.layer.2.output.dense.weight: 19.98%\n",
            "Sparsity in encoder.layer.2.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.attention.self.query.weight: 19.94%\n",
            "Sparsity in encoder.layer.3.attention.self.key.weight: 20.01%\n",
            "Sparsity in encoder.layer.3.attention.self.value.weight: 20.04%\n",
            "Sparsity in encoder.layer.3.attention.output.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.3.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.intermediate.dense.weight: 20.00%\n",
            "Sparsity in encoder.layer.3.output.dense.weight: 20.00%\n",
            "Sparsity in encoder.layer.3.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.attention.self.query.weight: 19.92%\n",
            "Sparsity in encoder.layer.4.attention.self.key.weight: 20.01%\n",
            "Sparsity in encoder.layer.4.attention.self.value.weight: 20.02%\n",
            "Sparsity in encoder.layer.4.attention.output.dense.weight: 19.99%\n",
            "Sparsity in encoder.layer.4.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.4.output.dense.weight: 19.98%\n",
            "Sparsity in encoder.layer.4.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.attention.self.query.weight: 19.96%\n",
            "Sparsity in encoder.layer.5.attention.self.key.weight: 20.11%\n",
            "Sparsity in encoder.layer.5.attention.self.value.weight: 19.95%\n",
            "Sparsity in encoder.layer.5.attention.output.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.5.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.intermediate.dense.weight: 20.07%\n",
            "Sparsity in encoder.layer.5.output.dense.weight: 20.00%\n",
            "Sparsity in encoder.layer.5.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.attention.self.query.weight: 20.00%\n",
            "Sparsity in encoder.layer.6.attention.self.key.weight: 19.99%\n",
            "Sparsity in encoder.layer.6.attention.self.value.weight: 19.98%\n",
            "Sparsity in encoder.layer.6.attention.output.dense.weight: 19.94%\n",
            "Sparsity in encoder.layer.6.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.intermediate.dense.weight: 19.98%\n",
            "Sparsity in encoder.layer.6.output.dense.weight: 20.04%\n",
            "Sparsity in encoder.layer.6.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.attention.self.query.weight: 19.93%\n",
            "Sparsity in encoder.layer.7.attention.self.key.weight: 20.02%\n",
            "Sparsity in encoder.layer.7.attention.self.value.weight: 20.01%\n",
            "Sparsity in encoder.layer.7.attention.output.dense.weight: 19.97%\n",
            "Sparsity in encoder.layer.7.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.7.output.dense.weight: 20.00%\n",
            "Sparsity in encoder.layer.7.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.attention.self.query.weight: 20.00%\n",
            "Sparsity in encoder.layer.8.attention.self.key.weight: 20.04%\n",
            "Sparsity in encoder.layer.8.attention.self.value.weight: 20.02%\n",
            "Sparsity in encoder.layer.8.attention.output.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.8.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.8.output.dense.weight: 19.99%\n",
            "Sparsity in encoder.layer.8.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.attention.self.query.weight: 20.03%\n",
            "Sparsity in encoder.layer.9.attention.self.key.weight: 20.03%\n",
            "Sparsity in encoder.layer.9.attention.self.value.weight: 20.05%\n",
            "Sparsity in encoder.layer.9.attention.output.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.9.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.intermediate.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.9.output.dense.weight: 19.97%\n",
            "Sparsity in encoder.layer.9.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.attention.self.query.weight: 19.99%\n",
            "Sparsity in encoder.layer.10.attention.self.key.weight: 20.02%\n",
            "Sparsity in encoder.layer.10.attention.self.value.weight: 20.02%\n",
            "Sparsity in encoder.layer.10.attention.output.dense.weight: 20.04%\n",
            "Sparsity in encoder.layer.10.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.intermediate.dense.weight: 20.02%\n",
            "Sparsity in encoder.layer.10.output.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.10.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.attention.self.query.weight: 20.04%\n",
            "Sparsity in encoder.layer.11.attention.self.key.weight: 19.98%\n",
            "Sparsity in encoder.layer.11.attention.self.value.weight: 20.00%\n",
            "Sparsity in encoder.layer.11.attention.output.dense.weight: 20.06%\n",
            "Sparsity in encoder.layer.11.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.11.output.dense.weight: 19.99%\n",
            "Sparsity in encoder.layer.11.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in pooler.dense.weight: 19.99%\n",
            "=== Pruned BERT Statistics ===\n",
            "Total parameters: 109,482,240\n",
            "Trainable parameters: 109,482,240\n",
            "Non-zero parameters: 92,273,662\n",
            "Model size: 417.64 MB\n",
            "Model sparsity: 15.64%\n",
            "=========================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize the model to float16\n",
        "print(\"\\nQuantizing model...\")\n",
        "fp16_model = convert_to_float16(pruned_model)\n",
        "print(\"Model quantized float16 successfully!\")\n",
        "\n",
        "# Print model statistics\n",
        "print(\"\\nGenerating float16 quantized model statistics...\")\n",
        "print_model_stats(\n",
        "    model=fp16_model,\n",
        "    model_name=\"Floa16 quantized BERT\",\n",
        ")"
      ],
      "metadata": {
        "id": "zRZd79WIlFWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03c8588-a441-4fe7-f4e6-ec82721bc3e7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Quantizing model...\n",
            "Model quantized float16 successfully!\n",
            "\n",
            "Generating float16 quantized model statistics...\n",
            "\n",
            "Layer-wise sparsity:\n",
            "--------------------------------------------------\n",
            "Sparsity in embeddings.word_embeddings.weight: 0.00%\n",
            "Sparsity in embeddings.position_embeddings.weight: 0.00%\n",
            "Sparsity in embeddings.token_type_embeddings.weight: 0.00%\n",
            "Sparsity in embeddings.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.attention.self.query.weight: 19.98%\n",
            "Sparsity in encoder.layer.0.attention.self.key.weight: 19.99%\n",
            "Sparsity in encoder.layer.0.attention.self.value.weight: 19.94%\n",
            "Sparsity in encoder.layer.0.attention.output.dense.weight: 20.05%\n",
            "Sparsity in encoder.layer.0.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.0.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.0.output.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.0.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.attention.self.query.weight: 19.95%\n",
            "Sparsity in encoder.layer.1.attention.self.key.weight: 20.06%\n",
            "Sparsity in encoder.layer.1.attention.self.value.weight: 20.01%\n",
            "Sparsity in encoder.layer.1.attention.output.dense.weight: 19.93%\n",
            "Sparsity in encoder.layer.1.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.1.intermediate.dense.weight: 20.02%\n",
            "Sparsity in encoder.layer.1.output.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.1.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.attention.self.query.weight: 20.02%\n",
            "Sparsity in encoder.layer.2.attention.self.key.weight: 20.04%\n",
            "Sparsity in encoder.layer.2.attention.self.value.weight: 20.00%\n",
            "Sparsity in encoder.layer.2.attention.output.dense.weight: 19.92%\n",
            "Sparsity in encoder.layer.2.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.2.intermediate.dense.weight: 19.98%\n",
            "Sparsity in encoder.layer.2.output.dense.weight: 19.98%\n",
            "Sparsity in encoder.layer.2.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.attention.self.query.weight: 19.94%\n",
            "Sparsity in encoder.layer.3.attention.self.key.weight: 20.01%\n",
            "Sparsity in encoder.layer.3.attention.self.value.weight: 20.04%\n",
            "Sparsity in encoder.layer.3.attention.output.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.3.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.3.intermediate.dense.weight: 20.00%\n",
            "Sparsity in encoder.layer.3.output.dense.weight: 20.00%\n",
            "Sparsity in encoder.layer.3.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.attention.self.query.weight: 19.92%\n",
            "Sparsity in encoder.layer.4.attention.self.key.weight: 20.01%\n",
            "Sparsity in encoder.layer.4.attention.self.value.weight: 20.02%\n",
            "Sparsity in encoder.layer.4.attention.output.dense.weight: 19.99%\n",
            "Sparsity in encoder.layer.4.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.4.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.4.output.dense.weight: 19.98%\n",
            "Sparsity in encoder.layer.4.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.attention.self.query.weight: 19.96%\n",
            "Sparsity in encoder.layer.5.attention.self.key.weight: 20.11%\n",
            "Sparsity in encoder.layer.5.attention.self.value.weight: 19.95%\n",
            "Sparsity in encoder.layer.5.attention.output.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.5.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.5.intermediate.dense.weight: 20.07%\n",
            "Sparsity in encoder.layer.5.output.dense.weight: 20.00%\n",
            "Sparsity in encoder.layer.5.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.attention.self.query.weight: 20.00%\n",
            "Sparsity in encoder.layer.6.attention.self.key.weight: 19.99%\n",
            "Sparsity in encoder.layer.6.attention.self.value.weight: 19.98%\n",
            "Sparsity in encoder.layer.6.attention.output.dense.weight: 19.94%\n",
            "Sparsity in encoder.layer.6.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.6.intermediate.dense.weight: 19.98%\n",
            "Sparsity in encoder.layer.6.output.dense.weight: 20.04%\n",
            "Sparsity in encoder.layer.6.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.attention.self.query.weight: 19.93%\n",
            "Sparsity in encoder.layer.7.attention.self.key.weight: 20.02%\n",
            "Sparsity in encoder.layer.7.attention.self.value.weight: 20.01%\n",
            "Sparsity in encoder.layer.7.attention.output.dense.weight: 19.97%\n",
            "Sparsity in encoder.layer.7.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.7.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.7.output.dense.weight: 20.00%\n",
            "Sparsity in encoder.layer.7.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.attention.self.query.weight: 20.00%\n",
            "Sparsity in encoder.layer.8.attention.self.key.weight: 20.04%\n",
            "Sparsity in encoder.layer.8.attention.self.value.weight: 20.02%\n",
            "Sparsity in encoder.layer.8.attention.output.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.8.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.8.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.8.output.dense.weight: 19.99%\n",
            "Sparsity in encoder.layer.8.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.attention.self.query.weight: 20.03%\n",
            "Sparsity in encoder.layer.9.attention.self.key.weight: 20.03%\n",
            "Sparsity in encoder.layer.9.attention.self.value.weight: 20.05%\n",
            "Sparsity in encoder.layer.9.attention.output.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.9.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.9.intermediate.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.9.output.dense.weight: 19.97%\n",
            "Sparsity in encoder.layer.9.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.attention.self.query.weight: 19.99%\n",
            "Sparsity in encoder.layer.10.attention.self.key.weight: 20.02%\n",
            "Sparsity in encoder.layer.10.attention.self.value.weight: 20.02%\n",
            "Sparsity in encoder.layer.10.attention.output.dense.weight: 20.04%\n",
            "Sparsity in encoder.layer.10.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.10.intermediate.dense.weight: 20.02%\n",
            "Sparsity in encoder.layer.10.output.dense.weight: 19.96%\n",
            "Sparsity in encoder.layer.10.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.attention.self.query.weight: 20.04%\n",
            "Sparsity in encoder.layer.11.attention.self.key.weight: 19.98%\n",
            "Sparsity in encoder.layer.11.attention.self.value.weight: 20.00%\n",
            "Sparsity in encoder.layer.11.attention.output.dense.weight: 20.06%\n",
            "Sparsity in encoder.layer.11.attention.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in encoder.layer.11.intermediate.dense.weight: 20.01%\n",
            "Sparsity in encoder.layer.11.output.dense.weight: 19.99%\n",
            "Sparsity in encoder.layer.11.output.LayerNorm.weight: 0.00%\n",
            "Sparsity in pooler.dense.weight: 19.99%\n",
            "=== Floa16 quantized BERT Statistics ===\n",
            "Total parameters: 109,482,240\n",
            "Trainable parameters: 109,482,240\n",
            "Non-zero parameters: 92,273,633\n",
            "Model size: 208.82 MB\n",
            "Model sparsity: 15.64%\n",
            "=========================\n"
          ]
        }
      ]
    }
  ]
}