{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaEpx6BfYmRc8UJFhLSBaz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierredantas/LLMCompress/blob/main/UoM_Pruning_Sparsity_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Techniques for Compressing Large Language Models in Python\n",
        "\n",
        "In this code, I am optimizing a lightweight large language model (~0.1B parameters, specifically BERT) by applying pruning to increase sparsity, followed by quantization to reduce memory usage and model size. Notably, this entire process is performed without requiring a GPU, making it highly accessible for environments with limited computational resources.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The results demonstrate the combined effects of pruning and quantization on the original model, focusing on parameter sparsity, memory reduction, and compression rate. Here's an analysis:\n",
        "\n",
        "### 1. **Pruning Results**:\n",
        "   - **Pruned Model Size and Memory**:\n",
        "     - The pruned model size and memory allocation remain identical to the original model. This happens because PyTorch's pruning mechanism uses a **mask-based approach**.\n",
        "     - **Mask-based pruning** applies sparsity by creating a binary `weight_mask` for each layer, which zeroes out specific weights but does not physically remove them. The pruned weights (`weight_orig`) still occupy memory, leading to no immediate reduction in size or memory usage.\n",
        "\n",
        "   - **Sparsity After Pruning**:\n",
        "     - The pruning process achieved a sparsity of **39.10%**, meaning 39.10% of the model's parameters were set to zero. However, because the weights are masked rather than removed, the total parameter count remains the same.\n",
        "\n",
        "### 2. **Quantization Results**:\n",
        "   - **Model Size Reduction**:\n",
        "     - Quantization physically reduced the parameter size from 32 bits (float32) to 8 bits (qint8), resulting in a **78.19% compression rate**. This reflects a significant reduction in memory usage and storage requirements.\n",
        "   \n",
        "   - **Memory Allocation**:\n",
        "     - The quantized model occupies only **22.77 MB** of memory compared to the original **417.64 MB**, achieving a **94.55% memory reduction rate**. This makes the model highly efficient for deployment on resource-constrained devices.\n",
        "\n",
        "### 3. **Overall Observations**:\n",
        "   - **Pruning** primarily increases sparsity, which can improve computational efficiency when supported by specialized hardware or frameworks capable of exploiting sparsity (e.g., sparse matrix multiplication). However, without removing pruned weights, the memory footprint remains unchanged.\n",
        "   - **Quantization** complements pruning by reducing the bit-width of parameters, leading to substantial reductions in model size and memory usage, irrespective of sparsity.\n",
        "\n",
        "### 4. **Recommendations**:\n",
        "   - **Mask Removal for Pruned Models**:\n",
        "     - To fully benefit from pruning, the pruned weights should be physically removed by reconstructing the model with reduced dimensions (e.g., excluding zeroed-out rows and columns). This would align the sparsity with the parameter count and memory usage.\n",
        "   - **Deploying Quantized Models**:\n",
        "     - The quantized model is ideal for deployment on edge devices or low-resource environments, given its significantly reduced size and memory requirements.\n",
        "\n",
        "### 5. **Key Takeaways**:\n",
        "   - The combination of **pruning** and **quantization** effectively balances sparsity, model size, and memory usage, making it a robust strategy for optimizing deep learning models.\n",
        "   - For maximum impact, frameworks that exploit both sparsity and quantization simultaneously should be used during inference.Introduction to LLM Model Compression\n",
        "\n",
        "Model compression is crucial for deploying large language models (LLMs) in resource-constrained environments. It aims to reduce model size and computational requirements while maintaining performance. This slideshow explores common techniques for LLM model compression using Python"
      ],
      "metadata": {
        "id": "ZDF6PU6j6rcE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26PPUIWp6YuC",
        "outputId": "82e4f975-b3dc-4f3c-d6dd-5bc304c74b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 109.48M parameters\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import AutoModel\n",
        "import time\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Print model size\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning - Removing Unimportant Weights\n",
        "\n",
        "Pruning involves removing less important weights from the model. This technique can significantly reduce model size with minimal impact on performance. We'll demonstrate magnitude-based pruning."
      ],
      "metadata": {
        "id": "bZ95iULI61eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply pruning and increase sparsity\n",
        "def prune_model(model, amount=0.5):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=amount)  # Apply pruning\n",
        "    return model"
      ],
      "metadata": {
        "id": "09ckgNhqioFY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to physically remove pruned weights\n",
        "def rebuild_pruned_model(model):\n",
        "    new_model = model.__class__(model.config)  # Create a new instance with the same configuration\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Extract pruned weight and bias\n",
        "            weight = module.weight.detach()\n",
        "            non_zero_rows = weight.abs().sum(dim=1) != 0\n",
        "            pruned_weight = weight[non_zero_rows]\n",
        "\n",
        "            if module.bias is not None:\n",
        "                bias = module.bias.detach()[non_zero_rows]\n",
        "            else:\n",
        "                bias = None\n",
        "\n",
        "            # Update the dimensions for the new Linear layer\n",
        "            in_features = module.in_features\n",
        "            out_features = pruned_weight.size(0)\n",
        "\n",
        "            # Create a new Linear layer with reduced dimensions\n",
        "            new_linear = nn.Linear(in_features, out_features, bias=(bias is not None))\n",
        "            new_linear.weight = nn.Parameter(pruned_weight)\n",
        "            if bias is not None:\n",
        "                new_linear.bias = nn.Parameter(bias)\n",
        "\n",
        "            # Replace the module in the new model\n",
        "            parent_module = new_model\n",
        "            sub_names = name.split(\".\")\n",
        "            for sub_name in sub_names[:-1]:\n",
        "                parent_module = getattr(parent_module, sub_name)\n",
        "            setattr(parent_module, sub_names[-1], new_linear)\n",
        "\n",
        "    return new_model\n",
        "\n",
        "# Function to calculate sparsity\n",
        "def calculate_sparsity(model):\n",
        "    total_params = 0\n",
        "    total_nonzero = 0\n",
        "\n",
        "    print(f\"{'Layer':<30} {'Non-Zero':<15} {'Total':<15} {'Sparsity (%)':<15}\")\n",
        "    print(\"-\" * 75)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            num_nonzero = (param != 0).sum().item()\n",
        "            num_total = param.numel()\n",
        "            sparsity = 100 * (1 - num_nonzero / num_total)\n",
        "\n",
        "            total_nonzero += num_nonzero\n",
        "            total_params += num_total\n",
        "\n",
        "            print(f\"{name:<30} {num_nonzero:<15} {num_total:<15} {sparsity:<15.2f}\")\n",
        "\n",
        "    overall_sparsity = 100 * (1 - total_nonzero / total_params)\n",
        "    print(\"-\" * 75)\n",
        "    print(f\"{'Total':<30} {total_nonzero:<15} {total_params:<15} {overall_sparsity:<15.2f}\")\n",
        "    return total_nonzero, total_params, overall_sparsity\n",
        "\n",
        "# Measure pruning time\n",
        "try:\n",
        "    # Original model details\n",
        "    original_model_size = sum(p.numel() for p in model.parameters()) / 1e6  # In millions\n",
        "    original_memory_size = sum(p.numel() for p in model.parameters()) * 4 / (1024**2)  # In MB\n",
        "\n",
        "    # Prune the model\n",
        "    start_time = time.time()\n",
        "    pruned_model = prune_model(model, amount=0.5)\n",
        "    pruned_model = rebuild_pruned_model(pruned_model)  # Rebuild with physically pruned weights\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate sparsity and memory\n",
        "    pruned_nonzero, pruned_total, pruned_sparsity = calculate_sparsity(pruned_model)\n",
        "    pruned_memory_size = pruned_nonzero * 4 / (1024**2)  # Memory for non-zero parameters\n",
        "\n",
        "    # Compression rate and time\n",
        "    pruning_time = end_time - start_time\n",
        "    compression_rate = ((original_model_size - pruned_total / 1e6) / original_model_size) * 100\n",
        "    memory_reduction_rate = ((original_memory_size - pruned_memory_size) / original_memory_size) * 100\n",
        "\n",
        "    # Output results\n",
        "    print(f\"Pruning completed successfully.\")\n",
        "    print(f\"Pruning time: {pruning_time:.2f} seconds\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Original model size: {original_model_size:.2f}M parameters\")\n",
        "    print(f\"Pruned model size: {pruned_total / 1e6:.2f}M parameters\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Original memory allocation: {original_memory_size:.2f} MB\")\n",
        "    print(f\"Pruned memory allocation: {pruned_memory_size:.2f} MB\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Sparsity after pruning: {pruned_sparsity:.2f}%\")\n",
        "    print(f\"Compression rate: {compression_rate:.2f}%\")\n",
        "    print(f\"Memory reduction rate: {memory_reduction_rate:.2f}%\")\n",
        "except Exception as e:\n",
        "    print(f\"Pruning failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxVre7THj4j5",
        "outputId": "fe7d9144-8706-4574-87da-b38048fa8f8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer                          Non-Zero        Total           Sparsity (%)   \n",
            "---------------------------------------------------------------------------\n",
            "embeddings.word_embeddings.weight 23440120        23440896        0.00           \n",
            "embeddings.position_embeddings.weight 393216          393216          0.00           \n",
            "embeddings.token_type_embeddings.weight 1536            1536            0.00           \n",
            "embeddings.LayerNorm.weight    768             768             0.00           \n",
            "encoder.layer.0.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.0.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.0.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.0.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.0.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.0.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.0.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.0.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.1.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.1.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.1.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.1.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.1.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.1.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.1.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.1.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.2.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.2.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.2.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.2.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.2.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.2.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.2.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.2.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.3.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.3.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.3.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.3.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.3.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.3.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.3.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.3.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.4.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.4.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.4.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.4.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.4.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.4.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.4.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.4.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.5.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.5.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.5.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.5.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.5.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.5.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.5.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.5.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.6.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.6.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.6.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.6.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.6.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.6.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.6.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.6.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.7.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.7.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.7.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.7.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.7.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.7.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.7.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.7.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.8.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.8.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.8.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.8.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.8.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.8.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.8.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.8.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.9.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.9.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.9.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.9.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.9.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.9.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.9.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.9.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.10.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.10.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.10.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.10.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.10.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.10.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.10.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.10.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.11.attention.self.query.weight 294912          589824          50.00          \n",
            "encoder.layer.11.attention.self.key.weight 294912          589824          50.00          \n",
            "encoder.layer.11.attention.self.value.weight 294912          589824          50.00          \n",
            "encoder.layer.11.attention.output.dense.weight 294912          589824          50.00          \n",
            "encoder.layer.11.attention.output.LayerNorm.weight 768             768             0.00           \n",
            "encoder.layer.11.intermediate.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.11.output.dense.weight 1179648         2359296         50.00          \n",
            "encoder.layer.11.output.LayerNorm.weight 768             768             0.00           \n",
            "pooler.dense.weight            294912          589824          50.00          \n",
            "---------------------------------------------------------------------------\n",
            "Total                          66616312        109379328       39.10          \n",
            "Pruning completed successfully.\n",
            "Pruning time: 12.31 seconds\n",
            "------------------------------------------------------\n",
            "Original model size: 109.48M parameters\n",
            "Pruned model size: 109.38M parameters\n",
            "------------------------------------------------------\n",
            "Original memory allocation: 417.64 MB\n",
            "Pruned memory allocation: 254.12 MB\n",
            "------------------------------------------------------\n",
            "Sparsity after pruning: 39.10%\n",
            "Compression rate: 0.09%\n",
            "Memory reduction rate: 39.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantization - Reducing Numerical Precision\n",
        "\n",
        "Quantization reduces the numerical precision of model weights and activations. This technique can significantly decrease model size and inference time. We'll demonstrate post-training static quantization."
      ],
      "metadata": {
        "id": "FmFIDKLy7Ct7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Hugging Face dynamic quantization\n",
        "def quantize_model(model):\n",
        "    model.eval()\n",
        "    quantized_model = torch.quantization.quantize_dynamic(\n",
        "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "    return quantized_model\n",
        "\n",
        "# Measure quantization time\n",
        "try:\n",
        "    # Assume `pruned_model` is obtained from the previous step\n",
        "    pruned_model_size = sum(p.numel() for p in pruned_model.parameters()) / 1e6  # In millions of parameters\n",
        "    pruned_memory_size = sum(p.numel() for p in pruned_model.parameters()) * 4 / (1024**2)  # In MB (4 bytes per parameter)\n",
        "\n",
        "    # Quantize the pruned model\n",
        "    start_time = time.time()\n",
        "    quantized_model = quantize_model(pruned_model)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate quantized model size\n",
        "    quantized_model_size = sum(p.numel() for p in quantized_model.parameters()) / 1e6  # In millions of parameters\n",
        "    quantized_memory_size = sum(p.numel() for p in quantized_model.parameters()) * 1 / (1024**2)  # In MB (1 byte per parameter for qint8)\n",
        "\n",
        "    # Quantization timing\n",
        "    quantization_time = end_time - start_time\n",
        "\n",
        "    # Calculate final compression rates relative to the original model\n",
        "    compression_rate = ((original_model_size - quantized_model_size) / original_model_size) * 100\n",
        "    memory_reduction_rate = ((original_memory_size - quantized_memory_size) / original_memory_size) * 100\n",
        "\n",
        "    # Output results\n",
        "    print(f\"Quantization of pruned model completed successfully.\")\n",
        "    print(f\"Quantization time: {quantization_time:.2f} seconds\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Original model size: {original_model_size:.2f}M parameters\")\n",
        "    print(f\"Pruned model size: {pruned_model_size:.2f}M parameters\")\n",
        "    print(f\"Quantized model size: {quantized_model_size:.2f}M parameters\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Original memory allocation: {original_memory_size:.2f} MB\")\n",
        "    print(f\"Pruned memory allocation: {pruned_memory_size:.2f} MB\")\n",
        "    print(f\"Quantized memory allocation: {quantized_memory_size:.2f} MB\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Final sparsity: {pruned_sparsity:.2f}%\")\n",
        "    print(f\"Final compression rate: {compression_rate:.2f}%\")\n",
        "    print(f\"Final memory reduction rate: {memory_reduction_rate:.2f}%\")\n",
        "except Exception as e:\n",
        "    print(f\"Quantization failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KygShgw7l0Gn",
        "outputId": "4c2d86eb-1e4a-4369-889f-9cb4808f29b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantization of pruned model completed successfully.\n",
            "Quantization time: 1.32 seconds\n",
            "------------------------------------------------------\n",
            "Original model size: 109.48M parameters\n",
            "Pruned model size: 109.48M parameters\n",
            "Quantized model size: 23.87M parameters\n",
            "------------------------------------------------------\n",
            "Original memory allocation: 417.64 MB\n",
            "Pruned memory allocation: 417.64 MB\n",
            "Quantized memory allocation: 22.77 MB\n",
            "------------------------------------------------------\n",
            "Final sparsity: 39.10%\n",
            "Final compression rate: 78.19%\n",
            "Final memory reduction rate: 94.55%\n"
          ]
        }
      ]
    }
  ]
}