{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZupJXbg5LX7p/ZPaipPvj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierredantas/LLMCompress/blob/main/Claude_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TuxBuDY1JpoV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch.nn.utils.prune as prune\n",
        "import os\n",
        "import numpy as np\n",
        "import psutil\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count the number of non-zero parameters in the model\"\"\"\n",
        "    total_params = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            total_params += (param != 0).sum().item()  # Only count non-zero parameters\n",
        "    return total_params\n",
        "\n",
        "def get_model_size(model):\n",
        "    \"\"\"Get model size in MB, considering only non-zero elements\"\"\"\n",
        "    # Save only non-zero elements\n",
        "    sparse_state_dict = {}\n",
        "    for name, param in model.state_dict().items():\n",
        "        if 'weight' in name:\n",
        "            non_zero_mask = param != 0\n",
        "            values = param[non_zero_mask]\n",
        "            indices = non_zero_mask.nonzero()\n",
        "            sparse_state_dict[f\"{name}_values\"] = values\n",
        "            sparse_state_dict[f\"{name}_indices\"] = indices\n",
        "        else:\n",
        "            sparse_state_dict[name] = param\n",
        "\n",
        "    # Save and get size\n",
        "    torch.save(sparse_state_dict, \"temp.p\")\n",
        "    size = os.path.getsize(\"temp.p\")/1e6\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "def get_memory_allocation():\n",
        "    \"\"\"Get current memory allocation in MB\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_mb = process.memory_info().rss / (1024 * 1024)\n",
        "    return memory_mb\n",
        "\n",
        "def get_size_mb(path):\n",
        "    \"\"\"Get model size in MB from path\"\"\"\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 * 1024)  # Convert bytes to MB\n",
        "\n",
        "def calculate_sparsity(model):\n",
        "    \"\"\"Calculate the sparsity (percentage of zero weights) in the model\"\"\"\n",
        "    zero_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            zero_count += torch.sum(param == 0).item()\n",
        "            total_count += param.numel()\n",
        "\n",
        "    return (zero_count / total_count) * 100 if total_count > 0 else 0\n",
        "\n",
        "def print_model_stats(model, model_path=None):\n",
        "    \"\"\"Print comprehensive statistics about the model\"\"\"\n",
        "    # Get all stats\n",
        "    params = count_parameters(model)\n",
        "    memory = get_memory_allocation()\n",
        "    sparsity = calculate_sparsity(model)\n",
        "\n",
        "    # Print all stats\n",
        "    print(f\"Number of parameters: {params/1e6:.2f}M\")\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        size = get_size_mb(model_path)\n",
        "        print(f\"Size: {size:.2f} MB\")\n",
        "    else:\n",
        "        # If no path provided or path doesn't exist, calculate size from the model directly\n",
        "        torch.save(model.state_dict(), \"temp.p\")\n",
        "        size = os.path.getsize(\"temp.p\") / (1024 * 1024)  # Convert to MB\n",
        "        os.remove(\"temp.p\")\n",
        "        print(f\"Size: {size:.2f} MB\")\n",
        "    print(f\"Memory allocation: {memory:.2f} MB\")\n",
        "    print(f\"Sparsity: {sparsity:.2f}%\")\n",
        "\n",
        "class SparseLinear(torch.nn.Linear):\n",
        "    def forward(self, input):\n",
        "        # Only use non-zero weights in forward pass\n",
        "        sparse_weight = self.weight * (self.weight != 0)\n",
        "        return torch.nn.functional.linear(input, sparse_weight, self.bias)\n",
        "\n",
        "def global_pruning(model, pruning_threshold):\n",
        "    \"\"\"Apply global pruning to the model with better memory management\"\"\"\n",
        "    # Instead of deepcopy, work on the model directly\n",
        "\n",
        "    # Collect weights for threshold calculation\n",
        "    all_weights = []\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            all_weights.append(module.weight.data.abs().view(-1))\n",
        "\n",
        "    # Calculate global threshold\n",
        "    all_weights = torch.cat(all_weights)\n",
        "    k = int(len(all_weights) * pruning_threshold)\n",
        "    threshold = torch.kthvalue(all_weights, k)[0]\n",
        "\n",
        "    # Free the temporary tensors\n",
        "    del all_weights\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "    # Apply pruning using the global threshold\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            # Create and apply mask\n",
        "            with torch.no_grad():  # Prevent storing gradient history\n",
        "                mask = (module.weight.data.abs() > threshold).float()\n",
        "                module.weight.data *= mask\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "fjHfdhQSJ1qO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load BERT model\n",
        "print(\"1. Loading BERT model...\")\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 2. Save original model\n",
        "print(\"\\n2. Saving original model...\")\n",
        "model.save_pretrained(\"original_model\")\n",
        "\n",
        "# 3. Print original model statistics\n",
        "print(\"\\n3. Original model statistics:\")\n",
        "print_model_stats(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-WeL4w5J95M",
        "outputId": "8d99f86e-96ad-4e2e-fd03-1360cc2e22bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading BERT model...\n",
            "\n",
            "2. Saving original model...\n",
            "\n",
            "3. Original model statistics:\n",
            "Number of parameters: 109.48M\n",
            "Size: 417.70 MB\n",
            "Memory allocation: 2242.09 MB\n",
            "Sparsity: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Objectives with pruning:\n",
        "1. reduce number of parameters (qty)\n",
        "2. Increase sparsity (%)\n",
        "\n",
        "#Non-objectives\n",
        "1. reduce memory allocation (MB)\n",
        "2. size (MB)\n"
      ],
      "metadata": {
        "id": "CNM0ZGPCiqIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Apply pruning\n",
        "pruning_threshold = 0.3  # This can be modified (e.g., 0.2 for 20%, 0.8 for 80%)\n",
        "print(f\"\\n4. Applying global pruning with threshold {pruning_threshold}...\")\n",
        "pruned_model = global_pruning(model, pruning_threshold)\n",
        "\n",
        "# 5. Save pruned model\n",
        "print(\"\\n5. Saving pruned model...\")\n",
        "pruned_model.save_pretrained(\"pruned_model\")\n",
        "\n",
        "# 6. Print pruned model statistics\n",
        "print(\"\\n6. Pruned model statistics:\")\n",
        "print_model_stats(pruned_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-484LcFVKR8",
        "outputId": "bd813f8b-a17a-4b74-f59e-6e996a312583"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4. Applying global pruning with threshold 0.3...\n",
            "\n",
            "5. Saving pruned model...\n",
            "\n",
            "6. Pruned model statistics:\n",
            "Number of parameters: 75.27M\n",
            "Size: 417.70 MB\n",
            "Memory allocation: 2158.53 MB\n",
            "Sparsity: 31.25%\n"
          ]
        }
      ]
    }
  ]
}