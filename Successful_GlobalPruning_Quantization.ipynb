{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhD/f+p9J5W2VNpMossi7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pierredantas/LLMCompress/blob/main/Successful_GlobalPruning_Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Techniques for Compressing Large Language Models in Python\n",
        "\n",
        "In this code, I am optimizing a lightweight large language model (~0.1B parameters, specifically BERT) by applying pruning to increase sparsity, followed by quantization to reduce memory usage and model size. Notably, this entire process is performed without requiring a GPU, making it highly accessible for environments with limited computational resources.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The results demonstrate the combined effects of pruning and quantization on the original model, focusing on parameter sparsity, memory reduction, and compression rate. Here's an analysis:\n",
        "\n",
        "### 1. **Pruning Results**:\n",
        "   - **Pruned Model Size and Memory**:\n",
        "     - The pruned model size and memory allocation remain identical to the original model. This happens because PyTorch's pruning mechanism uses a **mask-based approach**.\n",
        "     - **Mask-based pruning** applies sparsity by creating a binary `weight_mask` for each layer, which zeroes out specific weights but does not physically remove them. The pruned weights (`weight_orig`) still occupy memory, leading to no immediate reduction in size or memory usage.\n",
        "\n",
        "   - **Sparsity After Pruning**:\n",
        "     - The pruning process achieved a sparsity of **39.10%**, meaning 39.10% of the model's parameters were set to zero. However, because the weights are masked rather than removed, the total parameter count remains the same.\n",
        "\n",
        "### 2. **Quantization Results**:\n",
        "   - **Model Size Reduction**:\n",
        "     - Quantization physically reduced the parameter size from 32 bits (float32) to 8 bits (qint8), resulting in a **78.19% compression rate**. This reflects a significant reduction in memory usage and storage requirements.\n",
        "   \n",
        "   - **Memory Allocation**:\n",
        "     - The quantized model occupies only **22.77 MB** of memory compared to the original **417.64 MB**, achieving a **94.55% memory reduction rate**. This makes the model highly efficient for deployment on resource-constrained devices.\n",
        "\n",
        "### 3. **Overall Observations**:\n",
        "   - **Pruning** primarily increases sparsity, which can improve computational efficiency when supported by specialized hardware or frameworks capable of exploiting sparsity (e.g., sparse matrix multiplication). However, without removing pruned weights, the memory footprint remains unchanged.\n",
        "   - **Quantization** complements pruning by reducing the bit-width of parameters, leading to substantial reductions in model size and memory usage, irrespective of sparsity.\n",
        "\n",
        "### 4. **Recommendations**:\n",
        "   - **Mask Removal for Pruned Models**:\n",
        "     - To fully benefit from pruning, the pruned weights should be physically removed by reconstructing the model with reduced dimensions (e.g., excluding zeroed-out rows and columns). This would align the sparsity with the parameter count and memory usage.\n",
        "   - **Deploying Quantized Models**:\n",
        "     - The quantized model is ideal for deployment on edge devices or low-resource environments, given its significantly reduced size and memory requirements.\n",
        "\n",
        "### 5. **Key Takeaways**:\n",
        "   - The combination of **pruning** and **quantization** effectively balances sparsity, model size, and memory usage, making it a robust strategy for optimizing deep learning models.\n",
        "   - For maximum impact, frameworks that exploit both sparsity and quantization simultaneously should be used during inference.Introduction to LLM Model Compression\n",
        "\n",
        "Model compression is crucial for deploying large language models (LLMs) in resource-constrained environments. It aims to reduce model size and computational requirements while maintaining performance. This slideshow explores common techniques for LLM model compression using Python"
      ],
      "metadata": {
        "id": "ZDF6PU6j6rcE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "26PPUIWp6YuC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "from transformers import BertModel, BertConfig\n",
        "import time\n",
        "\n",
        "# Load the bert-base-uncased model\n",
        "config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "model = BertModel(config)\n",
        "\n",
        "# Store original model size\n",
        "original_model_size = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "original_memory_size = sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2)  # Assuming float32"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning - Removing Unimportant Weights\n",
        "\n",
        "Pruning involves removing less important weights from the model. This technique can significantly reduce model size with minimal impact on performance. We'll demonstrate magnitude-based pruning."
      ],
      "metadata": {
        "id": "bZ95iULI61eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the parameters to prune\n",
        "parameters_to_prune = []\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "        parameters_to_prune.append((module, 'weight'))\n",
        "\n",
        "# Apply global unstructured pruning\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.20\n",
        ")\n",
        "\n",
        "# Calculate pruned model size\n",
        "pruned_model_size = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "pruned_memory_size = sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2)\n",
        "\n",
        "# Calculate and print sparsity for each layer\n",
        "total_sparsity = 0\n",
        "total_elements = 0\n",
        "for module, param_name in parameters_to_prune:\n",
        "    param = getattr(module, param_name)\n",
        "    num_zeros = float(torch.sum(param == 0))\n",
        "    num_elements = float(param.nelement())\n",
        "    sparsity = 100. * num_zeros / num_elements\n",
        "    total_sparsity += num_zeros\n",
        "    total_elements += num_elements\n",
        "    print(f\"Sparsity in {module.__class__.__name__} {param_name}: {sparsity:.2f}%\")\n",
        "\n",
        "# Calculate and print global sparsity\n",
        "global_sparsity = 100. * total_sparsity / total_elements\n",
        "print(f\"Global sparsity: {global_sparsity:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxVre7THj4j5",
        "outputId": "7b792bc3-3510-4a0e-ddbe-829630b1f97a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in Linear weight: 20.03%\n",
            "Sparsity in Linear weight: 19.97%\n",
            "Sparsity in Linear weight: 20.00%\n",
            "Sparsity in Linear weight: 19.88%\n",
            "Sparsity in Linear weight: 20.03%\n",
            "Sparsity in Linear weight: 20.02%\n",
            "Sparsity in Linear weight: 20.03%\n",
            "Sparsity in Linear weight: 19.92%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 19.98%\n",
            "Sparsity in Linear weight: 19.97%\n",
            "Sparsity in Linear weight: 19.98%\n",
            "Sparsity in Linear weight: 20.08%\n",
            "Sparsity in Linear weight: 20.06%\n",
            "Sparsity in Linear weight: 19.99%\n",
            "Sparsity in Linear weight: 19.99%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 20.13%\n",
            "Sparsity in Linear weight: 19.99%\n",
            "Sparsity in Linear weight: 20.00%\n",
            "Sparsity in Linear weight: 19.95%\n",
            "Sparsity in Linear weight: 19.98%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 19.95%\n",
            "Sparsity in Linear weight: 20.07%\n",
            "Sparsity in Linear weight: 19.98%\n",
            "Sparsity in Linear weight: 20.04%\n",
            "Sparsity in Linear weight: 20.00%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 20.06%\n",
            "Sparsity in Linear weight: 20.07%\n",
            "Sparsity in Linear weight: 20.04%\n",
            "Sparsity in Linear weight: 19.89%\n",
            "Sparsity in Linear weight: 20.04%\n",
            "Sparsity in Linear weight: 19.99%\n",
            "Sparsity in Linear weight: 19.99%\n",
            "Sparsity in Linear weight: 20.03%\n",
            "Sparsity in Linear weight: 19.93%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 19.99%\n",
            "Sparsity in Linear weight: 19.99%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 19.94%\n",
            "Sparsity in Linear weight: 20.08%\n",
            "Sparsity in Linear weight: 19.92%\n",
            "Sparsity in Linear weight: 19.97%\n",
            "Sparsity in Linear weight: 20.02%\n",
            "Sparsity in Linear weight: 20.00%\n",
            "Sparsity in Linear weight: 20.05%\n",
            "Sparsity in Linear weight: 20.00%\n",
            "Sparsity in Linear weight: 19.96%\n",
            "Sparsity in Linear weight: 20.00%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 20.05%\n",
            "Sparsity in Linear weight: 20.04%\n",
            "Sparsity in Linear weight: 19.99%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 19.98%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 19.97%\n",
            "Sparsity in Linear weight: 20.02%\n",
            "Sparsity in Linear weight: 19.93%\n",
            "Sparsity in Linear weight: 19.95%\n",
            "Sparsity in Linear weight: 19.98%\n",
            "Sparsity in Linear weight: 19.99%\n",
            "Sparsity in Linear weight: 20.02%\n",
            "Sparsity in Linear weight: 20.05%\n",
            "Sparsity in Linear weight: 19.92%\n",
            "Sparsity in Linear weight: 19.98%\n",
            "Sparsity in Linear weight: 20.04%\n",
            "Sparsity in Linear weight: 20.01%\n",
            "Sparsity in Linear weight: 19.95%\n",
            "Global sparsity: 20.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantization - Reducing Numerical Precision\n",
        "\n",
        "Quantization reduces the numerical precision of model weights and activations. This technique can significantly decrease model size and inference time. We'll demonstrate post-training static quantization."
      ],
      "metadata": {
        "id": "FmFIDKLy7Ct7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Hugging Face dynamic quantization (modified)\n",
        "for module in model.modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        module.qconfig = torch.quantization.default_qconfig\n",
        "\n",
        "def quantize_model(target_model):\n",
        "    target_model.eval()\n",
        "    for module in target_model.modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            module.qconfig = torch.quantization.default_qconfig\n",
        "    torch.quantization.prepare(target_model, inplace=True)\n",
        "    torch.quantization.convert(target_model, inplace=True)\n",
        "    return target_model\n",
        "\n",
        "# Measure quantization time\n",
        "try:\n",
        "    # Quantize the pruned model\n",
        "    start_time = time.time()\n",
        "    final_quantized_model = quantize_model(model)  # Use the pruned model here\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate quantized model size\n",
        "    quantized_model_size = sum(p.numel() for p in final_quantized_model.parameters()) / 1e6\n",
        "    quantized_memory_size = sum(p.numel() for p in final_quantized_model.parameters()) * 1 / (1024 ** 2)  # Assuming int8\n",
        "\n",
        "    # Quantization timing\n",
        "    quantization_time = end_time - start_time\n",
        "\n",
        "    # Final compression rates\n",
        "    compression_rate = ((original_model_size - quantized_model_size) / original_model_size) * 100\n",
        "    memory_reduction_rate = ((original_memory_size - quantized_memory_size) / original_memory_size) * 100\n",
        "\n",
        "    # Output results\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Quantization of pruned model completed successfully.\")\n",
        "    print(f\"Quantization time: {quantization_time:.2f} seconds\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Original model size: {original_model_size:.2f}M parameters\")\n",
        "    print(f\"Pruned model size: {pruned_model_size:.2f}M parameters\")\n",
        "    print(f\"Quantized model size: {quantized_model_size:.2f}M parameters\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Original memory allocation: {original_memory_size:.2f} MB\")\n",
        "    print(f\"Pruned memory allocation: {pruned_memory_size:.2f} MB\")\n",
        "    print(f\"Quantized memory allocation: {quantized_memory_size:.2f} MB\")\n",
        "    print(f\"------------------------------------------------------\")\n",
        "    print(f\"Final sparsity: {global_sparsity:.2f}%\")  # Use the calculated global sparsity\n",
        "    print(f\"Final compression rate: {compression_rate:.2f}%\")\n",
        "    print(f\"Final memory reduction rate: {memory_reduction_rate:.2f}%\")\n",
        "except Exception as e:\n",
        "    print(f\"Quantization failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KygShgw7l0Gn",
        "outputId": "6fd308d0-9c41-47ab-ce39-81506069640e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/utils.py:407: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------\n",
            "Quantization of pruned model completed successfully.\n",
            "Quantization time: 0.87 seconds\n",
            "------------------------------------------------------\n",
            "Original model size: 109.48M parameters\n",
            "Pruned model size: 109.48M parameters\n",
            "Quantized model size: 23.87M parameters\n",
            "------------------------------------------------------\n",
            "Original memory allocation: 417.64 MB\n",
            "Pruned memory allocation: 417.64 MB\n",
            "Quantized memory allocation: 22.77 MB\n",
            "------------------------------------------------------\n",
            "Final sparsity: 20.00%\n",
            "Final compression rate: 78.19%\n",
            "Final memory reduction rate: 94.55%\n"
          ]
        }
      ]
    }
  ]
}